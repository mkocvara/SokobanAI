N
Successfully Trained Model
M
##########
#p.......#
#........#
#.##....x#
#..#.....#
#..#.b...#
#..#.....#
##########
I
Once the model has finished training, we can stop updating the Q-values and rely on what the model has learned. 

[THIS LEVEL SEEMS A LITTLE SUPERFLOUS, BUT PERHAPS WE COULD FURTHER EXPLAIN THAT THE MORE GENERATIONS THERE ARE, THE LONGER THE TRAINING TAKES, HENCE WHY FEWER IS BETTER? OTHERWISE, JUST MERGE IT WITH THE PREVIOUS LEVEL.]
Try reducing the number of generations as much as possible while still allowing the robot to win the game. Be careful not to reduce it too much!

[AFTER THE PREVIOUS LEVEL, THIS ONE IS SUPER EASY TO JUST DO WITH THE EXACT SAME SETTING. THAT ACTUALLY WORKS ALRIGHT WITH THE GOAL BEING TO REDUCE THE GENERATIONS HERE, AS IT WILL ALMOST CERTAINLY TAKE FEWER.]

[TROUBLE IS WITH THE EXPLORATION THRESHOLD THOUGH - THE LEVEL SHOULD HAVE THE SAME ONE AS THE ONE BEFORE, PRESUMABLY, BUT THAT MIGHT MAKE IT EASY TO DIP BELOW THAT NUMBER, LEAVING THE AI TRAINING REALLY RANDOM. IN THE END I WENT FROM 5500 GENERATIONS TO 4500, AT EXPLORATION THRESHOLD OF 3000, SO NOT TOO BAD.]